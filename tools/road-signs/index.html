<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="Hugo 0.80.0" />

  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="author" content="jsyang.ca" />
  <meta property="og:url" content="http://jsyang.ca/tools/road-signs/" />
  <link rel="canonical" href="http://jsyang.ca/tools/road-signs/" /><script type="application/ld+json">
  {
      "@context" : "http://schema.org",
      "@type" : "BlogPosting",
      "mainEntityOfPage": {
           "@type": "WebPage",
           "@id": "http:\/\/jsyang.ca\/"
      },
      "articleSection" : "tools",
      "name" : "Road signs",
      "headline" : "Road signs",
      "description" : "This was an undergraduate computer vision extra-curricular project.\nImage segmentation goal: finding text within am image of a road sign The growing abilities of video feed APIs allow for mobile and domain-specific programs that were previously out of reach. One application that I had in mind was video frame text extraction, which itself could be the foundation for magnification or text-to-speech capability that could aid the visually impaired, translation for foreign language signs and contextual advertising services.",
      "inLanguage" : "en-US",
      "author" : "jsyang.ca",
      "creator" : "jsyang.ca",
      "publisher": "jsyang.ca",
      "accountablePerson" : "jsyang.ca",
      "copyrightHolder" : "jsyang.ca",
      "copyrightYear" : "2010",
      "datePublished": "2010-04-10 00:00:00 \u002b0000 UTC",
      "dateModified" : "2010-04-10 00:00:00 \u002b0000 UTC",
      "url" : "http:\/\/jsyang.ca\/tools\/road-signs\/",
      "keywords" : [  ]
  }
</script>
<title>Road signs - jsyang.ca</title>
  <meta property="og:title" content="Road signs - jsyang.ca" />
  <meta property="og:type" content="article" />
  <meta name="description" content="This was an undergraduate computer vision extra-curricular project.
Image segmentation goal: finding text within am image of a road sign The growing abilities of video feed APIs allow for mobile and domain-specific programs that were previously out of reach. One application that I had in mind was video frame text extraction, which itself could be the foundation for magnification or text-to-speech capability that could aid the visually impaired, translation for foreign language signs and contextual advertising services." />

  <link
    rel="stylesheet"
    href="https://unpkg.com/flexboxgrid@6.3.1/dist/flexboxgrid.min.css"
  />
  <link
    rel="stylesheet"
    href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/2.10.0/github-markdown.min.css"
  />
  <link
    rel="stylesheet"
    href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/styles/tomorrow.min.css"
  />
  <link rel="stylesheet" href="/css/index.css">
  <link href="/index.xml" rel="alternate" type="application/rss+xml" title="jsyang.ca">
  
  <script>
    

    (function(undefined) {}).call('object' === typeof window && window || 'object' === typeof self && self || 'object' === typeof global && global || {});
  </script>


  
</head>


<body>
<article class="post " id="article">
    <div class="row">
        <div class="col-xs-12 col-md-8 col-md-offset-2 col-lg-6 col-lg-offset-3">
            <a href="/">
                <div class="head-line"></div>
            </a>
            <header class="post-header">
                <h1 class="post-title">Road signs</h1>
                <div class="row">
                    <div class="col-xs-6">
                        
                            <time class="post-date" datetime="2010-04-10 00:00:00 UTC">
                            10 Apr 2010
                            </time>
                        
                    </div>
                    <div class="col-xs-6">
                        <div class="post-author">
                            <a target="_blank" href="http://jsyang.ca/">&larr; Back to jsyang.ca</a>
                        </div>
                    </div>
                </div>
            </header>

            <div class="post-content markdown-body">
                <p>This was an undergraduate computer vision extra-curricular project.</p>
<h3 id="image-segmentation-goal-finding-text-within-am-image-of-a-road-sign">Image segmentation goal: finding text within am image of a road sign</h3>
<p>The growing abilities of video feed APIs allow for mobile and domain-specific programs
that were previously out of reach. One application that I had in mind was video frame
text extraction, which itself could be the foundation for magnification or text-to-speech
capability that could aid the visually impaired, translation for foreign language signs and
contextual advertising services. The idea is to help foreigners understand local signs.</p>
<blockquote>
<p>Update: Since this article was written, Google Goggles has been released and implements much
of the functionality described here</p>
</blockquote>
<p>Of course I can&rsquo;t do all of this alone; so the goal I&rsquo;d set out to meet was simple: develop an
algorithm for text detection using a Multi-layered Perceptron Neural Network (feed-forward
neural network). In addition, the entirety of the implementation was to serve as an easily-
modified and reusable scaffold for further experiments in neural networks.</p>
<h3 id="approach">Approach</h3>
<p>In creating the dataset (both training and test), Google Image Search returned 30 photos of
street signs. I then wrote a markup- generator to mark which areas contained text as part of
the supervised training. The &ldquo;correct&rdquo; regions are highlighted and these are used later to
check if the neural network predicted correctly. The overlay is a simple bitmap: non-zero
alpha bit for the text areas and zero alpha otherwise. All 30 images are marked by highlighting
the regions and saving the highlighting overlay.</p>
<p>Since many of the images contained extremely high levels of noise in both the compression
format as well as through non-important textures, a Gaussian blur had to be run twice through
each tile of the image before being fed into the sensitive edge-detection and energy filters.</p>
<p><img src="/images/road-signs/road-signs-1.png" alt="1"></p>
<p>The Energy filter was created empirically after much experimentation. Visually, I played around
with various schemes of luminance and displayed the result in the runFilters code that was
written until a satisfactory contrast detection filter was had. EnergyMap is particularly
sensitive to road signs. The Sobel operator was found through literature and applied in its 3x3
configuration.</p>
<p>Several different neural network layouts were tried. Each one was trained by breaking the image
into 8x8 tiles, feeding those tiles through a few filters and then through the nodes of
each NN. You can see the schemes for several of the NNs, as well as the object design, in the code
below:</p>
<p><img src="/images/road-signs/road-signs-2.png" alt="1"> <img src="/images/road-signs/road-signs-3.png" alt="1"></p>
<h3 id="implementation">Implementation</h3>
<p>The complete NN framework (supervised training set generator, NN training program, NN test
program) was written from scratch in JavaScript for maximum portability, allowing users to run
the software from any computer with a modern web-browser. Internet Explorer is not supported due
to lack of support for the canvas tag.</p>
<p>The first step in implementation was to feed a set images and a markup of the text regions on
those images. These two sets were then used in determining the weights for the nodes of our models.
Our test set consisted of 3 images that were part of the original validation set, this is because
the correct areas are already marked so testing the NN could be done automatically.
Below is a system schema.</p>
<p><img src="/images/road-signs/road-signs-4.png" alt="1"></p>
<p>The chosen activation functions between the Neurons in the NN should be sigmoid functions as is
standard with references. Due to this fact, backpropagation with the delta rule, for output layer
neurons, of which there are exactly 1 in each model, becomes:</p>
<pre><code>Delta output = (actual output)(1 - actual output)(desired output - actual output)
</code></pre>
<p>Deriving the derivative for the sigmoid function via chain rule, we get the delta rule for the hidden Neurons:</p>
<pre><code>Delta hidden = (actual output)(1 - actual output)( Sum(delta receiver neuron) dot product W receiver neuron for hidden)
</code></pre>
<p>These delta values are used in conjunction with the learning rate and the momentum of weight change
parameter to change the weights with each backpropagation. A typical sequence for a single 8x8
tile requires that we activate the NN first and then backpropagate the errors through the NN to
correct for mistakes in classification.</p>
<p>To build layers, we create the individual neurons and then link them together in the input calling
arrays for each layer, as well as through the receiver arrays. The NN creation code can be seen in <code>NN.js</code>.
This way, the network can be arbitrarily arranged and we can have skip-layer connections from one level to
any higher level.</p>
<h3 id="initial-model-parameters">Initial Model Parameters</h3>
<p>Determining the threshold of the output layers proved to be an immense challenge. There is no
standard way of solving this optimization problem so with the limited time we had to finish this project
we tried many thresholds through trial and error. Given more time, we would probably implement an automatic
cross-validation system that would give better estimates on a suitable threshold.</p>
<p>To be on the safe side, our implementation of the threshold estimate was highly lenient and rounded down
to the first significant digit minus 1 (0.033 ~ 0.02). To get this number, we gathered the
total output values for correct and incorrectly predicted tiles and then took the normal and geometric mean
of the two. Roughly our threshold formula is:</p>
<pre><code>t NN = Ugm - | Ugm - Um |

Threshold NN = (Floor( 100 t NN ) - 1) / 100
</code></pre>
<p>This works well enough as a starting point and was already a better threshold. Eventually through manual
cross-validation in Excel we reached a threshold of 0.02144445 which gave us the best threshold for the NN1
model. Thresholds for other models were obtained in a similar fashion.</p>
<h3 id="model-performance">Model Performance</h3>
<p>The models were evaluated using misclassification rate on the regions returned as text for the images.
Defining what constitutes as false positive and as false negative was a bit of a challenge. As a case in
point, borders of signs which also had high energy gradient and similar colour and style as the text they
enclose could be argued as being text or not. We made these assumptions in determining the misclassification
rate:</p>
<ul>
<li>Small text or illegible text that were not detected were classified as true negative</li>
<li>Borders detected as text will be classified as false positive</li>
<li>Other non-marked areas will be classified as true negative</li>
</ul>
<p>This was consistent with our markup images and thus the test set used in the runNN program gave us the
misclassification rates. Red tiles are predicted tiles which were incorrect, green tiles are predicted tiles
which are confirmed correct with our markup validation data. These are the results of each model being run
on the test set:</p>
<p><img src="/images/road-signs/road-signs-5.png" alt="1"></p>
<h3 id="in-short">In short</h3>
<p>We found that several factors influenced the ability to detect text. Some of the more obvious ones include
font size and colour of the text, amount of lighting in the picture, and even the texture and complexity
of the background. First off, our definition of what text was appeared to cause a lot of errors in the
generated model. We did not tightly control what passed for a &ldquo;text region&rdquo; other than a qualitative sense
of whether or not the region was legible. This would have had a major impact on our selection for training
images and markup. If we had used only a certain size font as the training set target then perhaps a better
result could be obtained. This same problem exists in the test set as it was part of our data set that we
had gathered.</p>
<p>Our training set could have also been much greater for the more complex models such as NN1 and NN2. No
doubt the ambiguities would have been less severe in swaying the classification of models of greater
layers. Furthermore, the parameters for the models raise questions about how they might be changed or
determined to a precision. Without a good threshold it is very hard to isolate the good classifications
from the false ones.</p>
<p>The generated system produced some reasonable results, an improved version could be well used as the basis
for a separate feature extraction or classification system. It could also be improved if more edge features
and image processing filters were available to the NN.</p>
<h3 id="links">Links</h3>
<ul>
<li>
<p><a href="http://en.wikipedia.org/wiki/Canny_edge_detector">Wikipedia: Canny edge detector</a></p>
</li>
<li>
<p><a href="http://homepages.inf.ed.ac.uk/rbf/HIPR2/sobel.htm">Sobel operator reference</a></p>
</li>
<li>
<p><a href="http://ecee.colorado.edu/~ecen4831/lectures/NNet3.html">NN backpropagation derivation</a></p>
</li>
<li>
<p><a href="http://www.generation5.org/content/2002/bp.asp">Backpropagation details</a></p>
</li>
<li>
<p><a href="http://www.cis.hut.fi/ahonkela/dippa/node41.html">Neural network notes</a></p>
</li>
<li>
<p><a href="http://www.learnartificialneuralnetworks.com/backpropagation.html">More on sigmoid back propagation for hidden units</a></p>
</li>
</ul>

            </div>
            
        </div>
    </div>
</article>

<script src="/js/highlight.pack.js"></script>
<script src="https://unpkg.com/quicklink@0.1.1/dist/quicklink.umd.js"></script>

<script>
  hljs.initHighlightingOnLoad();

  var posts = document.getElementById('posts-list');
  posts && quicklink({
    el: posts,
    priority: true,
  });
</script>



<script>
    
    var sc_project, sc_invisible=1, sc_security="29e623f2";
    if(location.hostname === 'localhost') {
        sc_project = 1337;
    } else {
        sc_project = 5678638;
    }
</script>
<script src="https://www.statcounter.com/counter/counter.js" async></script>
<noscript><img src="https://c.statcounter.com/5678638/0/29e623f2/1/"/></noscript>


</body>
</html>
